# PCA 完整理论推导


原来的数据很有可能各个维度之间是相关的，主成分分析的基本思想是将所有数据投影到一个子空间中并实现下列目标：
+ *最大化数据投影后的方差：* 所有数据在子空间中更为分散
+ *最小化投影造成的损失：* 损失的信息最小
**注意，这里降的是数据维数， 不是数据个数。**

投影概念介绍: [15. MIT线性代数---投影](https://zhuanlan.zhihu.com/p/45246414)
其他参考资料：[PCA（主成分分析）原理推导](https://zhuanlan.zhihu.com/p/84946694)

假定一组数据有$m$个样本，
$$X=\left[
    \begin{array}{c}
    {x^{(1)}}^T \\
    \ldots \\
    {x^{(m)}}^T
    \end{array}   
\right] \in \mathbb{R}^{m \times n}$$
每个样本有$n$维，${x^{(i)}}^T \in \mathbb{R}^{n}$。我们希望将样本降至$l$维。分别设计编码器函数和解码器函数：
$$f(x^{(i)})=c^{(i)}$$
$$\hat{x^{(i)}}=g(f(x^{(i)})) \approx x^{(i)}$$
为了简化计算， 我们希望解码器可以通过矩阵相乘得到
$$g(c)=Dc, D \in \mathbb{R}^{n \times l}, c\in \mathbb{R}^{l}$$
降维的目的是得到最优编码$c^{*}$
$$
\begin{aligned}
    c^{*} &= argmin_{c} ||x-g(c)||_2 = argmin_{c} ||x-g(c)||_2^2 \\
          &= argmin_{c} (x-g(c))^T(x-g(c))\\
          &= argmin_{c} -2x^T Dc + c^T c
\end{aligned}
$$
令其导数为0， 可以得到
$$c=D^T x$$
因此， 
$$\hat{x}=g(f(x))=DD^Tx$$
最优情况下，
$$D^{*}=argmin_{D}\sqrt{\sum_{i,j} (x_{j}^{(i)}-\hat{x}_{j}^{(i)})^2}$$

> 下面和书中不一样

对于$X$来讲，其表示形式为
$$X=\left[
    \begin{array}{c}
    {x^{(1)}}^T \\
    \ldots \\
    {x^{(m)}}^T
    \end{array}   
\right] \in \mathbb{R}^{m \times n}$$
均值$\overline{x}$为：
$$
\begin{aligned}
    \overline{x}&=\frac{1}{m}\sum_{i=1}^{m} x^{(i)}\\
                &=\frac{1}{N}\left( {x^{(1)}}^T, \ldots, {x^{(m)}}^T \right)_{n \times m} \left( 1, \ldots, 1 \right)^T_{m \times 1 } \\
                &= \frac{1}{m}X^T \mathbb{I}_{m \times 1}
\end{aligned}
$$

如果数据是1维的， 我们可以认为方差是
$$S=\frac{1}{m} \sum_{i=1}^{m} (x^{i}-\overline{x})^2 $$
如果扩展到多维， 我们可以类似地将方差$S$表示为：
$$
\begin{aligned}
    S_{m \times m} &= \frac{1}{m} \sum_{i=1}^m (\boldsymbol{x}^{(i)}-\overline{\boldsymbol{x}}) (\boldsymbol{x}^{(i)}-\overline{\boldsymbol{x}})^T \\
                   &= \frac{1}{m} \left( x^{(i)}-\overline{x}, \ldots, x^{(m)}-\overline{x} \right)_{n \times m} \left( x^{(i)}-\overline{x}, \ldots, x^{(m)}-\overline{x} \right)^T_{m \times n}\\
                   &=\frac{1}{m}(\left[ x^{(1)}, \ldots, x^{(m)} \right]-\left[ \overline{x}, \ldots, \overline{x} \right])(\left[ x^{(1)}, \ldots, x^{(m)} \right]-\left[ \overline{x}, \ldots, \overline{x} \right])^T \\
                   &= \frac{1}{m}\left( X^T - \frac{1}{m} X^T \mathbb{I}_{m \times 1} \mathbb{I}^T_{m \times 1} \right) \left( X^T - \frac{1}{m} X^T \mathbb{I}_{m \times 1} \mathbb{I}^T_{m \times 1} \right)^T \\
                   &= \frac{1}{m} X^T \left(I_{m \times m} - \frac{1}{m} \mathbb{I}_{m \times 1} \mathbb{I}^T_{m \times 1} \right) \left(I_{m \times m} - \frac{1}{m} \mathbb{I}_{m \times 1} \mathbb{I}^T_{m \times 1} \right)^T X \\
                   &= \frac{1}{m} X^T H_{m \times 1} H^T_{m \times 1} X = \frac{1}{m} X^T H X
\end{aligned}
$$

**我们可以令数据$X$在矩阵D上的投影方差最大化**


<font color='red' size=8> 注意： </font>我们可以先求$D\in \mathbb{R}^{n \times l}$中的某一列$w\in\mathbb{R}^{n \times 1}$
样本的投影的均值：
$$\mu = \frac{1}{m} \sum_{i=1}^m w^T x^{(i)} $$

因此样本投影后的方差表示为：
$$
\begin{aligned}
    \sigma^2 &= \frac{1}{m} \sum_{i=1}^m \left(w^T x^{(i)} - \mu \right)^2 \\
             &= \frac{1}{m} \sum_{i=1}^m \left(w^T x^{(i)} - \frac{1}{m} \sum_{i=1}^m w^T x^{(i)} \right)^2 \\
             &= \frac{1}{m} \sum_{i=1}^m \left(w^T x^{(i)} - w^T \left(\frac{1}{m} \sum_{i=1}^m x^{(i)} \right) \right)^2 \\
             &=  \frac{1}{m} \sum_{i=1}^m \left( w^T \left( x^{(i)}-\overline{x} \right) \right)^2 \\
             &= \frac{1}{m} \sum_{i=1}^m \left( w^T \left( x^{(i)}-\overline{x} \right) \right) \left( w^T \left( x^{(i)}-\overline{x} \right) \right)^T \\
             &= \frac{1}{m} \sum_{i=1}^m w^T \left( x^{(i)}-\overline{x} \right)\left( x^{(i)}-\overline{x} \right)^T w \\
             &= w^T \left[\frac{1}{m} \sum_{i=1}^m \left( x^{(i)}-\overline{x} \right)\left( x^{(i)}-\overline{x} \right)^T \right] w \\
             &= w^T S w
\end{aligned}
$$

因此优化目标为
$$\hat{w}=argmax_{w} w^T S w, \quad \text{subject to\quad} wTw=1$$
构造拉格朗日函数
$$L(w,\lambda)=w^T S w + \lambda(1-w^T w)$$
求偏导，并令其为0
$$\frac{\partial L(w.\lambda)}{\partial w}=2 S w - 2\lambda w =0$$
因此， $\lambda$是样本方差矩阵的特征值， $w$是向量。我们只要取最大的前$l$个特征值，以及对应的向量即可使投影方差最大化。

如果对样本方差矩阵进行奇异值分解，取前$l$个特征值及特征向量， 即可完成降维。
